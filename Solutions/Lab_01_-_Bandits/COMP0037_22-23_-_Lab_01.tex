\documentclass[twoside]{ucl_exam}
\usepackage[utf8]{inputenc}

\title{COMP0037 2022 / 2023 Robotic Systems\\Lab 01: Multi-Arm Bandits}
\author{COMP0037 Teaching Team}
\date{January 11, 2023}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{amsmath}
\usepackage[dvipsnames]{xcolor}
\usepackage[OT1]{fontenc} 

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}


\lstset{ %
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
  basicstyle=\footnotesize,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{red}\textit,    % comment style
  deletekeywords={...},            % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=tb,	                   	   % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{VioletRed}\bfseries,       % keyword style
  language=Python,                 % the language of the code (can be overrided per snippet)
  otherkeywords={*,...},           % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{red}, % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=1,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{maroon}, % string literal style
  tabsize=2,	                   % sets default tabsize to 2 spaces
  title=\lstname,                  % show the filename of files included with \lstinputlisting; also try caption instead of title
  columns=fixed                    % Using fixed column width (for e.g. nice alignment)
}

\begin{document}

\maketitle

\section*{Overview}

In this lab, we will explore the $k$-arm bandit problem. The bandit
problem is one of the most basic problems encountered in policy
learning. Despite its simplicity, it illustrates many extremely
important practical problems. In the lab you will implement a number
of algorithms for agents to solve the bandit problem. The lab also
presents an opportunity for you to become familiar with the GUI
environment and (slightly) with OpenAI Gym.

The lab itself consists of a lot of small tasks. Each one shouldn't
take much time to complete, but should give you a chance to try
coding, exploring the API and some of the characteristics. You might
notice that some files contain solutions to earlier tasks; since this
is an unmarked lab this is a deliberate feature and will allow you to
check your earlier work as you progress.

Note that sometimes that part a can occur later in the file than part
b.

The setup consists of the following:

\begin{enumerate}
\item There are a set of bandits, each of which corresponds to the slot machine.

\item Each bandit has its own (unknown to the agent)
  mean and covariance, and the rewards are sampled using a Gaussian distribution.

\item An environment is a collection of multiple bandits.

\item When an action is provided
to the environment, the appropriate bandit is selected and the reward is
returned. The agent is responsible for selecting arms, computing reward signals,
and working out the policy it will use.

\end{enumerate}

To carry out the lab, you will download and run some Python code and modify it
to implement you answers. The code provided runs, but does not do anything
interesting. Comments are provided in the code to suggest where changes should
be made. These comments are based on the changes we made to get the assigned
tasks work, but you might develop a solution which works differently.

\section*{Preliminaries}

The activities in this section focus on getting familiar with the basic system.

\begin{question}

The file \texttt{test\_bandit.py} contains code which will be used to test the
properties of an individual bandit running on its own. In this part, you only
need to modify this file.

\begin{subquestion}

Modify the code to create an instance of a \texttt{Bandit} object with mean 1.0
and standard deviation 2.0.

\end{subquestion}

\begin{subquestion}

Change the number of times the arm of the bandit is pulled and explore how the
mean and covariance change. What do notice about the rate of each? You might
want to increase by a factor of 10 each time. If you want, you can instrument
the code to measure the amount of time required as well.

\end{subquestion}

\begin{subquestion}

The code computes the mean reward using a ``batch'' in which we store all the
reward signals and compute the mean at the end. An alternative approach is to
use a recursive equation of the form:

\begin{equation*}
Q_{t+1}(a)=Q_t(a)+\frac{1}{t+1}\left(R_t(a)-Q_t(a)\right).
\end{equation*}

Implement this form and compare it with your batch calculation.

\emph{Hint:\/} Both the batch and resursive forms should produce near-identical
results (they shoudl agree to at least 8 decimal places) for both small and large
numbers of samples. If the two do not produce the same value, check your use of
indexing.

\end{subquestion}

\end{question}

\begin{question}

The file \texttt{test\_environment.py} contains code which will be used to test
the properties of multiple bandits in the environment. In this part, you only
need to modify this file.

\begin{subquestion}

With reference to the API for the \texttt{BanditEnvironment} class, create a
bandit environment with four bandits. These bandits should have the following
means and covariances:

\begin{center}
\begin{tabular}{|c|c|c|}\hline
Bandit&Mean&Standard deviation\\\hline
1&1&1\\
2&1&2\\
3&2&1\\
4&2&2\\\hline
\end{tabular}
\end{center}

\end{subquestion}

\begin{subquestion}

Modify the method \texttt{run\_bandits} so that it will iterate through each
bandit in the environment and compute the mean and covariance of the rewards.
Confirm these values are the same as the ones specified in the agent.

\emph{Hint:} Your code should \emph{only\/} use the input variables specified.
In particular, your solution must \emph{not\/} be hardwired to the number of
bandits in the environment. Rather, it must query the environment object and
determine them directly.

\end{subquestion}

\end{question}

\begin{question}

The final part is to implement an agent. You will modify the files
\texttt{random\_action\_agent.py} and \texttt{test\_agent.py}

\begin{subquestion}

Modify the implementation of \texttt{random\_action\_agent.py} so that it will
pick a valid action at random.

\emph{Hint:} Because we are using OpenAI gym to describe our environment, the
environment itself can be used to draw random actions. See
\url{https://gym.openai.com/docs/} for details.

\end{subquestion}

\begin{subquestion}

Modify \texttt{test\_agent.py} to add plots to show the sequence of actions
taken and the reward signals.

\emph{Hint:} Check \texttt{test\_bandit.py} to see how to create a
plot and draw values in it. To open multiple figures --- either in the
same or different windows --- check the
\href{https://matplotlib.org/stable/gallery/subplots_axes_and_figures/multiple_figs_demo.html}{matplotlib
  documentation}.

\end{subquestion}

\end{question}

\section*{Performance Measures}

In this question, you will implement two common methods for assessing
the performance of bandit agents. You will need to modify
\texttt{bandits/performance\_measures.py} and
\texttt{evaluate\_try\_them\_all.py}.

\begin{question}

\begin{subquestion}

Modify \texttt{evaluate\_try\_them\_all.py} to plot the percentage of
correct actions. Experiment with different numbers of initial trials
and see how this influences the number of correct actions. What do you
think the curves are showing you?

\end{subquestion}

\begin{subquestion}

Implement the method \texttt{compute\_regret} to compute the
regret. Your method should only use the \texttt{environment} and the
\texttt{reward\_history} and should not be hard coded.

\emph{Hint:} Check
\texttt{compute\_percentage\_of\_optimal\_actions\_selected} and the
public API of \texttt{BanditEnvironment}.

\end{subquestion}

\begin{subquestion}

Modify \texttt{evaluate\_try\_them\_all.py} to compute and plot the regret.
Experiment with different numbers of initial trials and see how this impacts the
regret.

\end{subquestion}

\end{question}

\section*{$\epsilon$-greedy Search}

\begin{question}

The simplest method for performing random exploration is to use the
$\epsilon$-Greedy algorithm. This is described in Slides 56--67 of
Lecture 02.

\begin{subquestion}

Modify the method \texttt{\_compute\_action} of \texttt{EpsilonGreedyAgent} to
implement the $\epsilon$-greedy exploration algorithm.

\emph{Hint:} Recall that you either need to pick a random agent or the agent
with the current best average action.

\end{subquestion}

\begin{subquestion}

Experiment with different values of $\epsilon$. What do you notice about the
behaviour of the regret and percentage optimal selection algorithms?

\end{subquestion}

\begin{subquestion}

One common way to improve $\epsilon$-greedy is to damp or change exploration
over time.

Experiment with the agent implemented in \texttt{DampedEpsilonGreedyAgent}. What
do you notice? 

\end{subquestion}

\end{question}

\section*{Upper Confidence Bound}

The UCB is another algorithm which attempts to find the optimal solution by
taking account of the uncertainty. It is described in the lectures slides
73--84.

\begin{question}

\begin{subquestion}

Modify \texttt{upper\_confidence\_bound\_agent.py} to implement the UCB algorithm.

\end{subquestion}

\begin{subquestion}

Explore with different values of $c$ to see the impact on the rewards and the
percentage of optimal actions taken.
 
\end{subquestion}
\end{question}



\end{document}

